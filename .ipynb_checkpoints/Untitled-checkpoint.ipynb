{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232a2aa0-54dd-4c75-9990-8447c5fb2842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auyin11\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import  get_logger, seed_everything\n",
    "from cfg import Cfg\n",
    "from train_predict.train import train_loop\n",
    "from utils import get_score\n",
    "import os\n",
    "import wandb\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "# print(f\"torch.__version__: {torch.__version__}\")\n",
    "# print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "# print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "# TODO: what is it\n",
    "# %env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "def get_result(oof_df):\n",
    "    labels = oof_df['score'].values\n",
    "    preds = oof_df['pred'].values\n",
    "    score = get_score(labels, preds)\n",
    "    logger.info(f'Score: {score:<.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71775b46-0182-46b8-a9c7-b64856495dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92ca3f-8c66-43c8-83a0-8b31019c7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fb1937-a352-4e39-b846-7a1e815e7dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init TrainDataset ##############\n",
      "self.cfg.aaaa: 1\n",
      "['CV', 'Dir', 'Model', 'Train', '_wandb_kernel', 'aaaa', 'apex', 'bbbb', 'cccc', 'competition', 'dddd', 'debug', 'max_len', 'print_freq', 'seed', 'tokenizer', 'train', 'wandb']\n",
      "###############\n",
      "['CV', 'Dir', 'Model', 'Train', '_wandb_kernel', 'aaaa', 'apex', 'bbbb', 'cccc', 'competition', 'dddd', 'debug', 'max_len', 'print_freq', 'seed', 'tokenizer', 'train', 'wandb']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\auyin11\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\auyin11\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\auyin11\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\auyin11\\PycharmProjects\\competition_patent\\gen_data\\dataset.py\", line 46, in __getitem__\n    print(f'cfg.aaaa: {self.cfg.aaaa}')\nAttributeError: type object 'Cfg' has no attribute 'aaaa'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset,\n\u001b[0;32m     14\u001b[0m                           batch_size\u001b[38;5;241m=\u001b[39mCfg\u001b[38;5;241m.\u001b[39mTrain\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m     15\u001b[0m                           shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     16\u001b[0m                           num_workers\u001b[38;5;241m=\u001b[39mCfg\u001b[38;5;241m.\u001b[39mTrain\u001b[38;5;241m.\u001b[39mnum_workers, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m([var \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mcfg) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m var\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs:::::: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1223\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1250\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m--> 457\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\auyin11\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\auyin11\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\auyin11\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\auyin11\\PycharmProjects\\competition_patent\\gen_data\\dataset.py\", line 46, in __getitem__\n    print(f'cfg.aaaa: {self.cfg.aaaa}')\nAttributeError: type object 'Cfg' has no attribute 'aaaa'\n"
     ]
    }
   ],
   "source": [
    "from cfg import Cfg\n",
    "Cfg.dddd = 1111111111\n",
    "from gen_data.dataset import TrainDataset\n",
    "\n",
    "Cfg.aaaa = '1'\n",
    "Cfg.max_len = 100\n",
    "tokenizer = AutoTokenizer.from_pretrained(Cfg.Model.pretrained_model)\n",
    "tokenizer.save_pretrained(os.path.join(Cfg.Dir.output, 'tokenizer'))\n",
    "Cfg.tokenizer = tokenizer\n",
    "df = pd.DataFrame({'text': ['1', '2', '3', '1', '2'] * 1000, 'score': [0, 0, 1, 1, 0] * 1000})\n",
    "\n",
    "train_dataset = TrainDataset(Cfg, df)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=Cfg.Train.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=Cfg.Train.num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "print([var for var in dir(train_dataset.cfg) if not var.startswith('__')])\n",
    "\n",
    "for step, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f'inputs:::::: {inputs}')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82910cb4-e73d-41e4-b082-ba48e4ea240a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc69927-0f8f-4496-afd1-ac3946b8a473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f2e8b5d-3b07-4052-b6cc-6b321d3f2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a9cf48b-0eae-4a81-b0d7-7045c4a963c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cfg:\n",
    "    \n",
    "    # TODO\n",
    "    aaaa = 1\n",
    "\n",
    "    train = True\n",
    "    seed = 42\n",
    "    # TODO\n",
    "    wandb = False\n",
    "    competition = 'PPPM'\n",
    "    _wandb_kernel = 'nakama'\n",
    "    debug = False\n",
    "    apex = True\n",
    "    print_freq = 100\n",
    "\n",
    "    bbbb = 10000000000000000\n",
    "\n",
    "    # dir and path\n",
    "    # TODO\n",
    "    dir_output = 'output'\n",
    "    path_train_log = os.path.join(dir_output, 'train_predict.log')\n",
    "\n",
    "    dir_data = 'data'\n",
    "\n",
    "    # training\n",
    "    num_workers = 4\n",
    "    batch_size = 16\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5\n",
    "    num_warmup_steps = 0\n",
    "    epochs = 3\n",
    "    max_len = 512\n",
    "\n",
    "    # CV\n",
    "    n_fold = 4\n",
    "    trn_fold = [0, 1, 2, 3]\n",
    "\n",
    "    # Model\n",
    "    pretrained_model = \"microsoft/deberta-v3-large\"\n",
    "\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 2e-5\n",
    "    min_lr = 1e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    fc_dropout = 0.2\n",
    "    target_size = 1\n",
    "    weight_decay = 0.01\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    # def __init__(self):\n",
    "    #     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8fbca8c-c9db-4413-8994-067d8c55dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cfg.zzzz = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e46b18d6-d0fd-4a5d-8e39-5a6a0d7bf2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3024582-7050-4f69-a2a2-db72fee7207a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.target_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8b1abc3-844d-47af-8d4f-61c8923f1a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_wandb_kernel',\n",
       " 'aaaa',\n",
       " 'apex',\n",
       " 'batch_scheduler',\n",
       " 'batch_size',\n",
       " 'bbbb',\n",
       " 'betas',\n",
       " 'competition',\n",
       " 'debug',\n",
       " 'decoder_lr',\n",
       " 'dir_data',\n",
       " 'dir_output',\n",
       " 'encoder_lr',\n",
       " 'epochs',\n",
       " 'eps',\n",
       " 'fc_dropout',\n",
       " 'gradient_accumulation_steps',\n",
       " 'max_grad_norm',\n",
       " 'max_len',\n",
       " 'min_lr',\n",
       " 'n_fold',\n",
       " 'num_cycles',\n",
       " 'num_warmup_steps',\n",
       " 'num_workers',\n",
       " 'path_train_log',\n",
       " 'pretrained_model',\n",
       " 'print_freq',\n",
       " 'scheduler',\n",
       " 'seed',\n",
       " 'target_size',\n",
       " 'train',\n",
       " 'trn_fold',\n",
       " 'wandb',\n",
       " 'weight_decay',\n",
       " 'zzzz']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34541e55-b754-46b9-b666-5880f651bca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.zzzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bcaa03-4c28-4b8b-9321-071f96ce1967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "competition_patent",
   "language": "python",
   "name": "competition_patent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
