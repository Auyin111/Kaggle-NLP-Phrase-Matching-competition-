{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f50600-c142-4e30-8bb0-e5730eb05e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\auyin11\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.11.0\n",
      "tokenizers.__version__: 0.10.3\n",
      "transformers.__version__: 4.16.2\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from utils import  get_logger, seed_everything\n",
    "from cfg import Cfg\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "# TODO: what is it\n",
    "# %env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device}')\n",
    "logger = get_logger(Cfg.Dir.train_log)\n",
    "seed_everything(seed=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0e5b09-64df-4297-8fd5-dbb99af0fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(Cfg.Dir.data, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(Cfg.Dir.data, 'test.csv'))\n",
    "\n",
    "# preprocess\n",
    "# TODO: create text column\n",
    "df_train['text'] = df_train.anchor + '[SEP]' + df_train.target + ['SEP']\n",
    "df_test['text'] = df_test.anchor + '[SEP]' + df_test.target + ['SEP']\n",
    "\n",
    "df_train['score_map'] = df_train.score.map({0:0, 0.25:1, 0.5:2, 0.75:3, 1:4})\n",
    "\n",
    "# cv split\n",
    "fold = StratifiedKFold(n_splits=Cfg.CV.n_fold, shuffle=True, random_state=Cfg.seed)\n",
    "\n",
    "for n, (train_index, val_index) in enumerate(fold.split(df_train, df_train['score_map'])):\n",
    "    df_train.loc[val_index, 'fold'] = n\n",
    "df_train['fold'] = df_train.fold.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4a8636-b50d-444f-a261-5598f6c727af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cfg.Dir.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ef0cc-5f60-4bd8-ad10-0343d7132e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b84974-c345-4ff8-b9ea-ff6196baeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ce9fbe-4df6-43aa-9587-4e6acdd30ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def prepare_input(cfg, text):\n",
    "    print('in prepare_input: !!!!!!!!!!!!')\n",
    "    print([var for var in dir(cfg) if not var.startswith('__')])\n",
    "    print(f'cfg.aaaa: {cfg.aaaa}')\n",
    "    print('!!!!!!!!!!!!')\n",
    "\n",
    "\n",
    "\n",
    "    # quit()\n",
    "\n",
    "    inputs = cfg.tokenizer(text,\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=cfg.Train.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        print('init TrainDataset ##############')\n",
    "        print(f'self.cfg.aaaa: {self.cfg.aaaa}')\n",
    "        self.cfg.cccc = 100000000000\n",
    "        print([var for var in dir(self.cfg) if not var.startswith('__')])\n",
    "\n",
    "        print('###############')\n",
    "\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df['score'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        print('when get item ----------------------')\n",
    "        print([var for var in dir(self.cfg) if not var.startswith('__')])\n",
    "        print(f'cfg.aaaa: {self.cfg.aaaa}')\n",
    "        print('-----------------------')\n",
    "\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367feb01-0fbb-4c9d-bc4c-93e1b0cddcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_input(cfg, text):\n",
    "#     print('in prepare_input: !!!!!!!!!!!!')\n",
    "#     print([var for var in dir(cfg) if not var.startswith('__')])\n",
    "#     print(f'cfg.aaaa: {cfg.aaaa}')\n",
    "#     print('!!!!!!!!!!!!')\n",
    "\n",
    "\n",
    "\n",
    "#     # quit()\n",
    "\n",
    "#     inputs = cfg.tokenizer(text,\n",
    "#                            add_special_tokens=True,\n",
    "#                            max_length=cfg.max_len,\n",
    "#                            padding=\"max_length\",\n",
    "#                            return_offsets_mapping=False)\n",
    "#     for k, v in inputs.items():\n",
    "#         inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "#     return inputs\n",
    "\n",
    "\n",
    "# class TrainDataset(Dataset):\n",
    "#     def __init__(self, cfg, df):\n",
    "#         print('init TrainDataset ##############')\n",
    "#         print([var for var in dir(cfg) if not var.startswith('__')])\n",
    "#         print(f'cfg.aaaa: {cfg.aaaa}')\n",
    "#         print('###############')\n",
    "\n",
    "\n",
    "#         self.cfg = cfg\n",
    "#         self.texts = df['text'].values\n",
    "#         self.labels = df['score'].values\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         print('when get item ----------------------')\n",
    "#         print([var for var in dir(self.cfg) if not var.startswith('__')])\n",
    "#         print('-----------------------')\n",
    "\n",
    "#         inputs = prepare_input(self.cfg, self.texts[item])\n",
    "#         label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "#         return inputs, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8dac5-52b4-4001-a8e3-ce756203a0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c671a-d9b6-45fc-9ad5-8a36c791d3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23e733-3718-433c-b00c-2b5841fb8000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84650d9e-84cd-42df-84d7-2be6bdd664b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init TrainDataset ##############\n",
      "self.cfg.aaaa: 1\n",
      "['CV', 'Dir', 'Model', 'Train', '_wandb_kernel', 'aaaa', 'apex', 'bbbb', 'cccc', 'competition', 'debug', 'max_len', 'print_freq', 'seed', 'tokenizer', 'train', 'wandb']\n",
      "###############\n",
      "['CV', 'Dir', 'Model', 'Train', '_wandb_kernel', 'aaaa', 'apex', 'bbbb', 'cccc', 'competition', 'debug', 'max_len', 'print_freq', 'seed', 'tokenizer', 'train', 'wandb']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 8724, 21176, 3408, 20732) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset,\n\u001b[0;32m     12\u001b[0m                           batch_size\u001b[38;5;241m=\u001b[39mCfg\u001b[38;5;241m.\u001b[39mTrain\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m     13\u001b[0m                           shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m                           num_workers\u001b[38;5;241m=\u001b[39mCfg\u001b[38;5;241m.\u001b[39mTrain\u001b[38;5;241m.\u001b[39mnum_workers, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m([var \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mcfg) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m var\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs:::::: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1207\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1163\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1165\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\competition_patent\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1024\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1023\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 8724, 21176, 3408, 20732) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "from cfg import Cfg\n",
    "\n",
    "Cfg.aaaa = '1'\n",
    "Cfg.max_len = 100\n",
    "tokenizer = AutoTokenizer.from_pretrained(Cfg.Model.pretrained_model)\n",
    "tokenizer.save_pretrained(os.path.join(Cfg.Dir.output, 'tokenizer') )\n",
    "Cfg.tokenizer = tokenizer\n",
    "df = pd.DataFrame({'text':['1', '2', '3', '1', '2'] * 1000, 'score':[0,0,1,1,0]* 1000})\n",
    "\n",
    "train_dataset = TrainDataset(Cfg, df)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=Cfg.Train.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=Cfg.Train.num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "\n",
    "print([var for var in dir(train_dataset.cfg) if not var.startswith('__')])\n",
    "\n",
    "\n",
    "for step, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f'inputs:::::: {inputs}')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79291e-5df2-452e-8786-f8e190ee267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88ed02-185e-448e-a31e-06b16a6ed056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([var for var in dir(Cfg) if not var.startswith('__')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf363274-3103-41d0-8c01-061c7041dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cfg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349a873-adff-475f-9818-cef888f0b803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23234119-000a-4dac-8bf3-05223d0bfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f'inputs:::::: {inputs}')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed4e2e-0cae-4539-9f71-2afdf126121c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f2e8a6-0416-4393-9c32-240e7ac50783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e73c7f-40dd-4209-bd39-1defa863e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bc134-c9c9-4749-b76e-8dcf43d7071e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6a513-fd77-48c5-805f-ecd882e96229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f906b3-c167-4012-8926-8cda4ebf69d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd51eb-cbaf-4eb4-812d-8f5589981492",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e98ab-95d6-47aa-83fe-6ff12ebef0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00883d-f1d6-404e-b941-bd354e299e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1fe110-0466-4acd-9bbb-123946f56d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900774b7-d16d-49b6-aa5a-c70039ff3386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b5488-0c85-4743-bffa-952d8d1b3030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bfc97f-9163-49bf-aea4-1b19bedaf1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_predict.train import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a46c6-9ed3-4704-aadb-1b468ea89728",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Cfg.Model.pretrained_model)\n",
    "tokenizer.save_pretrained(os.path.join(Cfg.Dir.output, 'tokenizer') )\n",
    "Cfg.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e067ff0-546f-4dbd-99f4-5c3b7859c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb00ae-1221-421e-9938-2fe96d2609f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(oof_df):\n",
    "    labels = oof_df['score'].values\n",
    "    preds = oof_df['pred'].values\n",
    "    score = get_score(labels, preds)\n",
    "    logger.info(f'Score: {score:<.4f}')\n",
    "\n",
    "if Cfg.train:\n",
    "    oof_df = pd.DataFrame()\n",
    "    for fold in range(Cfg.CV.n_fold):\n",
    "        if fold in Cfg.CV.trn_fold:\n",
    "            _oof_df = train_loop(df_train, fold, Cfg, logger)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            logger.info(f\"========== fold: {fold} result ==========\")\n",
    "            get_result(_oof_df)\n",
    "    oof_df = oof_df.reset_index(drop=True)\n",
    "    logger.info(f\"========== CV ==========\")\n",
    "    get_result(oof_df)\n",
    "    oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "\n",
    "if Cfg.wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407f53f-0993-4157-9368-8ce12faa4dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be22ec-52c9-4a95-9ad7-6ded5f0ad847",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c70df5-deb9-412a-aea6-79ebde49deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d5403-2874-4c6f-8d28-7aae22b3a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train['fold'] = train['fold'].astype(int)\n",
    "# display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a614c3-6f40-4a29-a45f-db7966874850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c799c73-6777-478e-94d9-b97c8aa22f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['fold', 'score_map']).id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c023a-ca8e-4e13-b4f4-6ae05fcd66f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d8b76-5229-4568-b926-09841bb7ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237edf69-1893-4099-917d-6ab09b1c80a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "competition_patent",
   "language": "python",
   "name": "competition_patent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
